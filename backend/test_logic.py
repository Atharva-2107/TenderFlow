# import os
# from dotenv import load_dotenv
# from flashrank import Ranker # Must be imported before LangChain FlashrankRerank
# from llama_parse import LlamaParse
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_community.embeddings import FastEmbedEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_classic.retrievers import ContextualCompressionRetriever
# from langchain_classic.retrievers.document_compressors import FlashrankRerank
# from langchain_core.documents import Document

# load_dotenv()

# def advanced_rag_pipeline(pdf_path):
#     # 1. High-Fidelity Parsing (LlamaParse)
#     print(f"\n1Ô∏è‚É£  STARTING: Parsing {pdf_path}...")
#     parser = LlamaParse(api_key=os.getenv("LLAMA_CLOUD_API_KEY"), result_type="markdown")
#     llama_docs = parser.load_data(pdf_path)
    
#     # üõ†Ô∏è THE FIX: Convert LlamaIndex Documents to LangChain Documents
#     # This maps Llama's '.text' to LangChain's '.page_content'
#     langchain_docs = [
#         Document(page_content=d.text, metadata=d.metadata or {}) 
#         for d in llama_docs
#     ]
#     print(f"‚úÖ PARSED. Converted {len(langchain_docs)} pages to LangChain format.")

#     # 2. Smart Chunking (Recursive)
#     print("\n2Ô∏è‚É£  STARTING: Chunking text...")
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=150)
#     # We pass the converted 'langchain_docs' here
#     docs = text_splitter.split_documents(langchain_docs)
#     print(f"‚úÖ CHUNKED. Created {len(docs)} chunks.")

#     # 3. Local Vector Engine (FastEmbed + FAISS)
#     print("\n3Ô∏è‚É£  STARTING: FastEmbed (Indexing)...")
#     embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")
#     vectorstore = FAISS.from_documents(docs, embeddings)
#     base_retriever = vectorstore.as_retriever(search_kwargs={"k": 30})

#     # 4. Advanced Reranking (Contextual Compression)
#     print("\n4Ô∏è‚É£  STARTING: FlashRank (Reranking)...")
#     compressor = FlashrankRerank(model="ms-marco-MultiBERT-L-12")
#     compression_retriever = ContextualCompressionRetriever(
#         base_compressor=compressor, 
#         base_retriever=base_retriever
#     )

#     # 5. Execution
#     query = "What is Financial Management."
#     print(f"\nüîé SEARCHING for: '{query}'")
    
#     # CHANGE: Use .invoke(query) instead of .get_relevant_documents(query)
#     compressed_docs = compression_retriever.invoke(query)

#     print(f"‚úÖ SUCCESS. Found {len(compressed_docs)} highly relevant chunks.")
    
#     # Print the top results with their new Relevance Scores
#     for i, doc in enumerate(compressed_docs[:3]):
#         score = doc.metadata.get('relevance_score', 'N/A')
#         # Format the score to 4 decimal places if it's a number
#         formatted_score = f"{score:.4f}" if isinstance(score, float) else score
        
#         print(f"\nüèÜ RANK {i+1} (Score: {formatted_score})")
#         print(f"CONTENT: {doc.page_content[:400]}...")
#         print("-" * 50)
        
#     return compressed_docs

# if __name__ == "__main__":
#     test_file = "sample_tender.pdf"
#     if os.path.exists(test_file):
#         advanced_rag_pipeline(test_file)
#     else:
#         print(f"‚ùå ERROR: File '{test_file}' not found.")


import os
from dotenv import load_dotenv
from flashrank import Ranker
from groq import Groq  # üß† The Brain
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import FastEmbedEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import FlashrankRerank
from langchain_core.documents import Document

load_dotenv()

# --- NEW: Initialize Groq Client ---
groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))

def generate_summary_with_groq(user_query, retrieved_chunks):
    """
    Sends the top ranked chunks to Llama-3.3 on Groq for a final answer.
    """
    print(f"\nüß† GENERATING: Sending {len(retrieved_chunks)} chunks to Llama-3.3...")
    
    # 1. Combine all chunk text into one big context block
    context_text = "\n\n".join([doc.page_content for doc in retrieved_chunks])
    
    # 2. Build the Strict Prompt
    system_prompt = """
    You are a Senior Technical Tender Analyst. Your job is to extract precise information 
    from the provided tender documents to answer the user's question.
    
    Rules:
    - Answer ONLY based on the provided Context.
    - If the answer is not in the context, say "Data not found in the provided documents."
    - Be detailed and professional. Use bullet points where appropriate.
    - Cite specific section names or page numbers if available in the text.
    """
    
    user_prompt = f"""
    Context Data:
    {context_text}
    
    User Question: 
    {user_query}
    
    Provide a detailed, structured response:
    """

    # 3. Call Groq API
    chat_completion = groq_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        model="llama-3.3-70b-versatile", # The big gun model
        temperature=0.3, # Keep it factual, low creativity
    )
    
    return chat_completion.choices[0].message.content

def advanced_rag_pipeline(pdf_path):
    # 1. High-Fidelity Parsing
    print(f"\n1Ô∏è‚É£  STARTING: Parsing {pdf_path}...")
    parser = LlamaParse(api_key=os.getenv("LLAMA_CLOUD_API_KEY"), result_type="markdown")
    llama_docs = parser.load_data(pdf_path)
    
    # Conversion Step (Llama -> LangChain)
    langchain_docs = [
        Document(page_content=d.text, metadata=d.metadata or {}) 
        for d in llama_docs
    ]
    print(f"‚úÖ PARSED. Converted {len(langchain_docs)} pages to LangChain format.")

    # 2. Smart Chunking (Recursive) -> BIGGER CHUNKS
    print("\n2Ô∏è‚É£  STARTING: Chunking text...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = text_splitter.split_documents(langchain_docs)
    print(f"‚úÖ CHUNKED. Created {len(docs)} chunks.")

    # 3. Local Vector Engine (FastEmbed + FAISS)
    print("\n3Ô∏è‚É£  STARTING: FastEmbed (Indexing)...")
    embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")
    vectorstore = FAISS.from_documents(docs, embeddings)
    print("‚úÖ INDEXED. Vector Database created in memory.") # <--- Added Tick Mark!

    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 20}) # Increased to 30

    # 4. Advanced Reranking (Contextual Compression)
    print("\n4Ô∏è‚É£  STARTING: FlashRank (Reranking)...")
    compressor = FlashrankRerank(model="ms-marco-MultiBERT-L-12")
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor, 
        base_retriever=base_retriever
    )

    # 5. Execution
    query = "What is Financial Management."
    print(f"\nüîé SEARCHING for: '{query}'")
    
    # Get top 5 highly relevant chunks (Increased from 3)
    compressed_docs = compression_retriever.invoke(query)[:5]
    print(f"‚úÖ RETRIEVAL COMPLETE. Found {len(compressed_docs)} Gold Chunks.")
    
    # 6. Final Generation (Groq)
    final_answer = generate_summary_with_groq(query, compressed_docs)
    
    print("\n" + "="*50)
    print("ü§ñ FINAL GENERATED ANSWER:")
    print("="*50)
    print(final_answer)
    print("="*50)
    
    return final_answer

if __name__ == "__main__":
    test_file = "sample_tender.pdf"
    if os.path.exists(test_file):
        advanced_rag_pipeline(test_file)
    else:
        print(f"‚ùå ERROR: File '{test_file}' not found.")